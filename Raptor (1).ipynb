{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9pRnrPEPn06"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install -U datasets sentence-transformers umap-learn scikit-learn faiss-cpu huggingface_hub numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")  # hidden secret\n",
        "client = InferenceClient(api_key=HF_TOKEN)\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Explain RAPTOR in simple terms.\"}],\n",
        "    max_tokens=200,\n",
        ")\n",
        "\n",
        "print(resp.choices[0].message.content)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-L0ulHyQPofg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# RAPTOR-only on PubMedQA (public healthcare dataset)\n",
        "# - Builds RAPTOR tree for ONE selected PubMedQA example\n",
        "# - Runs 1â€“2 queries against the collapsed tree\n",
        "# - Uses Colab Secret HF_TOKEN via userdata.get(\"HF_TOKEN\")\n",
        "# =========================\n",
        "\n",
        "# !pip -q install -U datasets sentence-transformers umap-learn scikit-learn faiss-cpu huggingface_hub\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import umap\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import faiss\n",
        "\n",
        "from huggingface_hub import InferenceClient\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Tokenization + chunking\n",
        "# ----------------------------\n",
        "def simple_tokens(text: str) -> List[str]:\n",
        "    return re.findall(r\"\\w+|[^\\w\\s]\", text, flags=re.UNICODE)\n",
        "\n",
        "def chunk_text_sentence_aware(text: str, max_tokens: int = 100) -> List[str]:\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    sents = re.split(r\"(?<=[\\.\\?\\!])\\s+\", text)\n",
        "\n",
        "    chunks, cur, cur_len = [], [], 0\n",
        "    for s in sents:\n",
        "        s = s.strip()\n",
        "        if not s:\n",
        "            continue\n",
        "        tok_len = len(simple_tokens(s))\n",
        "        if tok_len == 0:\n",
        "            continue\n",
        "\n",
        "        # hard split too-long sentence\n",
        "        if tok_len > max_tokens:\n",
        "            if cur:\n",
        "                chunks.append(\" \".join(cur).strip())\n",
        "                cur, cur_len = [], 0\n",
        "            toks = simple_tokens(s)\n",
        "            for i in range(0, len(toks), max_tokens):\n",
        "                piece = \" \".join(toks[i:i + max_tokens]).strip()\n",
        "                if piece:\n",
        "                    chunks.append(piece)\n",
        "            continue\n",
        "\n",
        "        # pack into chunks\n",
        "        if cur_len + tok_len <= max_tokens:\n",
        "            cur.append(s)\n",
        "            cur_len += tok_len\n",
        "        else:\n",
        "            chunks.append(\" \".join(cur).strip())\n",
        "            cur = [s]\n",
        "            cur_len = tok_len\n",
        "\n",
        "    if cur:\n",
        "        chunks.append(\" \".join(cur).strip())\n",
        "\n",
        "    return [c for c in chunks if c.strip()]\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 1) GMM selection by BIC\n",
        "# ----------------------------\n",
        "def bic_best_gmm(X: np.ndarray, k_min: int = 2, k_max: int = 12, seed: int = 42):\n",
        "    n = len(X)\n",
        "    if n < 3:\n",
        "        return None\n",
        "\n",
        "    k_max = min(k_max, n - 1)\n",
        "    k_max = min(k_max, max(2, n // 2))\n",
        "    if k_max < k_min:\n",
        "        return None\n",
        "\n",
        "    best_gmm, best_bic = None, float(\"inf\")\n",
        "    for k in range(k_min, k_max + 1):\n",
        "        gmm = GaussianMixture(n_components=k, covariance_type=\"full\", random_state=seed)\n",
        "        gmm.fit(X)\n",
        "        bic = gmm.bic(X)\n",
        "        if bic < best_bic:\n",
        "            best_bic, best_gmm = bic, gmm\n",
        "    return best_gmm\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Node structure (RAPTOR)\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class Node:\n",
        "    node_id: int\n",
        "    text: str\n",
        "    level: int\n",
        "    children: List[int] = field(default_factory=list)\n",
        "    embedding: Optional[np.ndarray] = None\n",
        "    token_len: int = 0\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Hugging Face LLM wrapper\n",
        "# ----------------------------\n",
        "class HFChatLLM:\n",
        "    def __init__(self, api_key: str, model: str = \"mistralai/Mistral-7B-Instruct-v0.2\"):\n",
        "        self.client = InferenceClient(api_key=api_key)\n",
        "        self.model = model\n",
        "\n",
        "    def chat(self, user_msg: str, max_tokens: int = 256) -> str:\n",
        "        resp = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": user_msg}],\n",
        "            max_tokens=max_tokens,\n",
        "        )\n",
        "        return resp.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 4) RAPTOR implementation\n",
        "# ----------------------------\n",
        "class Raptor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        llm: HFChatLLM,\n",
        "        embedder: SentenceTransformer,\n",
        "        seed: int = 42,\n",
        "        umap_dim: int = 10,\n",
        "        base_neighbors: int = 15,\n",
        "    ):\n",
        "        self.llm = llm\n",
        "        self.embedder = embedder\n",
        "        self.seed = seed\n",
        "        self.umap_dim = umap_dim\n",
        "        self.base_neighbors = base_neighbors\n",
        "        self.nodes: Dict[int, Node] = {}\n",
        "        self.next_id = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.nodes = {}\n",
        "        self.next_id = 0\n",
        "\n",
        "    def _new_node(self, text: str, level: int, children: List[int]) -> int:\n",
        "        nid = self.next_id\n",
        "        self.next_id += 1\n",
        "        self.nodes[nid] = Node(\n",
        "            node_id=nid,\n",
        "            text=text,\n",
        "            level=level,\n",
        "            children=children,\n",
        "            token_len=len(simple_tokens(text))\n",
        "        )\n",
        "        return nid\n",
        "\n",
        "    def _embed_nodes(self, node_ids: List[int]) -> None:\n",
        "        texts = [self.nodes[i].text for i in node_ids]\n",
        "        embs = self.embedder.encode(texts, normalize_embeddings=True, show_progress_bar=False)\n",
        "        for i, e in zip(node_ids, embs):\n",
        "            self.nodes[i].embedding = e.astype(np.float32)\n",
        "\n",
        "    def _make_umap(self, n_samples: int):\n",
        "        # Adaptive + init=\"random\" avoids spectral k>=N error\n",
        "        nn = min(self.base_neighbors, max(2, n_samples - 1))\n",
        "        ncomp = min(self.umap_dim, max(2, n_samples - 1))\n",
        "        return umap.UMAP(\n",
        "            n_components=ncomp,\n",
        "            n_neighbors=nn,\n",
        "            metric=\"cosine\",\n",
        "            random_state=self.seed,\n",
        "            init=\"random\",\n",
        "            low_memory=True,\n",
        "        )\n",
        "\n",
        "    def summarize_cluster(self, child_texts: List[str]) -> str:\n",
        "        joined = \"\\n\\n\".join(child_texts)[:12000]\n",
        "        prompt = (\n",
        "            \"You are summarizing a cluster of medical research passages.\\n\"\n",
        "            \"Produce 5â€“8 bullet points capturing key facts, methods, results, and entities.\\n\"\n",
        "            \"Be information-dense and avoid fluff.\\n\\n\"\n",
        "            f\"PASSAGES:\\n{joined}\\n\\nSUMMARY:\"\n",
        "        )\n",
        "        return self.llm.chat(prompt, max_tokens=240)\n",
        "\n",
        "    def build_tree(\n",
        "        self,\n",
        "        document_text: str,\n",
        "        chunk_tokens: int = 100,\n",
        "        max_levels: int = 3,\n",
        "        min_cluster_size: int = 3,\n",
        "        min_points_for_clustering: int = 8,  # small-N stop fix\n",
        "    ) -> List[int]:\n",
        "        self.reset()\n",
        "\n",
        "        chunks = chunk_text_sentence_aware(document_text, max_tokens=chunk_tokens)\n",
        "        leaf_ids = [self._new_node(c, level=0, children=[]) for c in chunks]\n",
        "        if not leaf_ids:\n",
        "            return []\n",
        "\n",
        "        self._embed_nodes(leaf_ids)\n",
        "\n",
        "        current = leaf_ids\n",
        "        level = 0\n",
        "\n",
        "        while level < max_levels:\n",
        "            if len(current) < max(min_cluster_size, min_points_for_clustering):\n",
        "                break\n",
        "\n",
        "            X = np.stack([self.nodes[i].embedding for i in current], axis=0)\n",
        "            reducer = self._make_umap(n_samples=len(current))\n",
        "            Xr = reducer.fit_transform(X)\n",
        "\n",
        "            gmm = bic_best_gmm(Xr, k_min=2, k_max=min(12, len(current) - 1), seed=self.seed)\n",
        "            if gmm is None:\n",
        "                break\n",
        "\n",
        "            labels = gmm.predict(Xr)\n",
        "            clusters: Dict[int, List[int]] = {}\n",
        "            for nid, lab in zip(current, labels):\n",
        "                clusters.setdefault(int(lab), []).append(nid)\n",
        "\n",
        "            if len(clusters) <= 1:\n",
        "                break\n",
        "\n",
        "            parents, new_parent_ids = [], []\n",
        "            for kids in clusters.values():\n",
        "                if len(kids) < min_cluster_size:\n",
        "                    parents.extend(kids)\n",
        "                    continue\n",
        "\n",
        "                summary = self.summarize_cluster([self.nodes[k].text for k in kids])\n",
        "                pid = self._new_node(summary, level=level + 1, children=kids)\n",
        "                parents.append(pid)\n",
        "                new_parent_ids.append(pid)\n",
        "\n",
        "            if not new_parent_ids:\n",
        "                break\n",
        "\n",
        "            self._embed_nodes(new_parent_ids)\n",
        "            current = parents\n",
        "            level += 1\n",
        "\n",
        "        return current\n",
        "\n",
        "    def collapsed_tree_retrieval(\n",
        "        self,\n",
        "        query: str,\n",
        "        top_k: int = 12,\n",
        "        max_tokens: int = 900,\n",
        "    ) -> Tuple[str, List[int]]:\n",
        "        if not self.nodes:\n",
        "            return \"\", []\n",
        "\n",
        "        q = self.embedder.encode([query], normalize_embeddings=True, show_progress_bar=False)[0].astype(np.float32)\n",
        "\n",
        "        node_ids = list(self.nodes.keys())\n",
        "        embs = np.stack([self.nodes[i].embedding for i in node_ids], axis=0).astype(np.float32)\n",
        "\n",
        "        index = faiss.IndexFlatIP(embs.shape[1])\n",
        "        index.add(embs)\n",
        "\n",
        "        _, idxs = index.search(q.reshape(1, -1), min(len(node_ids), top_k * 6))\n",
        "\n",
        "        chosen, total = [], 0\n",
        "        for j in idxs[0]:\n",
        "            nid = node_ids[int(j)]\n",
        "            tlen = self.nodes[nid].token_len\n",
        "            if total + tlen > max_tokens:\n",
        "                continue\n",
        "            chosen.append(nid)\n",
        "            total += tlen\n",
        "            if len(chosen) >= top_k:\n",
        "                break\n",
        "\n",
        "        context = \"\\n\\n\".join([self.nodes[i].text for i in chosen])\n",
        "        return context, chosen\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Dataset formatting (PubMedQA)\n",
        "# ----------------------------\n",
        "def pubmedqa_to_document(example) -> str:\n",
        "    ctx = example.get(\"context\", {}) or {}\n",
        "    contexts = ctx.get(\"contexts\", [])\n",
        "    if isinstance(contexts, list):\n",
        "        ctx_text = \" \".join([str(x) for x in contexts if x])\n",
        "    else:\n",
        "        ctx_text = str(contexts)\n",
        "    long_answer = str(example.get(\"long_answer\", \"\") or \"\")\n",
        "    return f\"{ctx_text}\\n\\n{long_answer}\".strip()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Answer helper (optional)\n",
        "# ----------------------------\n",
        "def answer_with_llm(llm: HFChatLLM, question: str, context: str, max_tokens: int = 220) -> str:\n",
        "    prompt = (\n",
        "        \"Answer the medical question using ONLY the provided context.\\n\"\n",
        "        \"Return:\\n\"\n",
        "        \"Decision: yes / no / maybe\\n\"\n",
        "        \"Justification: 2-4 sentences grounded in context\\n\\n\"\n",
        "        f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{question}\\n\\nANSWER:\"\n",
        "    )\n",
        "    return llm.chat(prompt, max_tokens=max_tokens)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 7) Run RAPTOR on 1 example + 1 or 2 queries\n",
        "# ----------------------------\n",
        "def run_raptor_pubmedqa_single_example(\n",
        "    example_idx: int = 0,\n",
        "    queries: Optional[List[str]] = None,\n",
        "    chunk_tokens: int = 100,\n",
        "    max_levels: int = 3,\n",
        "    top_k: int = 12,\n",
        "    max_context_tokens: int = 900,\n",
        "):\n",
        "    HF_TOKEN = userdata.get(\"HF_TOKEN\")  # âœ… hidden secret\n",
        "    if HF_TOKEN is None:\n",
        "        raise ValueError(\"HF_TOKEN not found in Colab Secrets. Add it in the ðŸ”‘ Secrets panel and allow notebook access.\")\n",
        "\n",
        "    llm = HFChatLLM(api_key=HF_TOKEN, model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "    embedder = SentenceTransformer(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
        "    raptor = Raptor(llm=llm, embedder=embedder)\n",
        "\n",
        "    # Load ONE example (public healthcare dataset)\n",
        "    ds = load_dataset(\"pubmed_qa\", \"pqa_labeled\", split=\"train\")\n",
        "    ex = ds[example_idx]\n",
        "\n",
        "    question = ex[\"question\"]\n",
        "    gold = (ex.get(\"final_decision\") or \"\").strip()\n",
        "    doc_text = pubmedqa_to_document(ex)\n",
        "\n",
        "    print(\"=\" * 100)\n",
        "    print(\"DATASET: PubMedQA (pqa_labeled)\")\n",
        "    print(\"Example index:\", example_idx)\n",
        "    print(\"Question:\", question)\n",
        "    print(\"Gold label:\", gold)\n",
        "    print(\"Doc length (chars):\", len(doc_text))\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    # Build RAPTOR tree for this document\n",
        "    roots = raptor.build_tree(\n",
        "        doc_text,\n",
        "        chunk_tokens=chunk_tokens,\n",
        "        max_levels=max_levels,\n",
        "        min_cluster_size=3,\n",
        "        min_points_for_clustering=8\n",
        "    )\n",
        "    print(\"RAPTOR built.\")\n",
        "    print(\"Total nodes:\", len(raptor.nodes))\n",
        "    print(\"Root node ids:\", roots)\n",
        "\n",
        "    # Default: use dataset question + one extra query (2 queries total)\n",
        "    if queries is None:\n",
        "        queries = [\n",
        "            question,\n",
        "            \"What is the main conclusion and evidence from this study?\"\n",
        "        ]\n",
        "\n",
        "    # Run 1â€“2 queries\n",
        "    for qi, q in enumerate(queries[:2], start=1):\n",
        "        print(\"\\n\" + \"-\" * 100)\n",
        "        print(f\"QUERY {qi}: {q}\")\n",
        "\n",
        "        context, chosen = raptor.collapsed_tree_retrieval(\n",
        "            q, top_k=top_k, max_tokens=max_context_tokens\n",
        "        )\n",
        "\n",
        "        print(f\"\\nRetrieved nodes: {len(chosen)} (showing first 1500 chars of context)\")\n",
        "        print(context[:1500])\n",
        "\n",
        "        # Optional: ask LLM to answer using retrieved context\n",
        "        ans = answer_with_llm(llm, q, context)\n",
        "        print(\"\\nLLM Answer:\")\n",
        "        print(ans)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Run it\n",
        "# ----------------------------\n",
        "run_raptor_pubmedqa_single_example(\n",
        "    example_idx=0,   # change to try different PubMedQA sample\n",
        "    queries=None,    # or pass your own list: [\"...\", \"...\"]\n",
        "    chunk_tokens=100,\n",
        "    max_levels=3,\n",
        "    top_k=12,\n",
        "    max_context_tokens=900,\n",
        ")"
      ],
      "metadata": {
        "id": "q_ruBfzaaw7Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}